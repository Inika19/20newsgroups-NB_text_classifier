{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:14:40.254976Z",
     "start_time": "2020-02-13T18:14:38.975693Z"
    }
   },
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:14:40.913293Z",
     "start_time": "2020-02-13T18:14:40.902375Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '20_newsgroups/' #will be used to access the directory containing the data\n",
    "folders = sorted(os.listdir(os.path.join(data_dir))) #folders is a (sorted) list of the folder names in the data\n",
    "\n",
    "if '.DS_Store' in folders:\n",
    "    del folders[0]\n",
    "# used it to remove '.DS_Store' item at folders[0] (as list is sorted, this file appears at 0 index due to its name) \n",
    "# when code is run on MacOS (not required for windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Dictionary\n",
    "\n",
    "In this cell, *each file in each folder is opened, read and its contents are added to data as a string*\n",
    "where data is a dictionary of the form { folder1 : \\[doc1,doc2,....,doc1000\\] , folder2 : \\[doc1,doc2,doc3,....\\] ..,folder20:\\[..\\]}\n",
    "i.e. doc1 is a string containing entire contents of the first file in folder1, doc2 is a string containing contents \n",
    "of the second file in folder1 and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:14:50.693668Z",
     "start_time": "2020-02-13T18:14:44.713813Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for folder in folders:\n",
    "    data[folder] = []\n",
    "    for file in os.listdir(os.path.join(data_dir,folder)):\n",
    "        with open(os.path.join(data_dir,folder,file),encoding='latin-1') as opened_file:\n",
    "            data[folder].append(opened_file.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T11:02:50.416463Z",
     "start_time": "2019-09-17T11:02:48.386Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# x = 0\n",
    "# for i in range(len(folders)):\n",
    "#     y = len((data[folders[i]]))\n",
    "#     print(folders[i], str(y))\n",
    "#     x+=y\n",
    "# print(x)\n",
    "\n",
    "#prints no of files in each folder and finally the total no of files in all 20 folders (x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T11:02:50.418899Z",
     "start_time": "2019-09-17T11:02:48.394Z"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating stop words\n",
    "\n",
    "Stop words will be ignored when encountered in the files as they are read. Here I've used stop words from nltk library, a file of common stop words in english that I found online and added to it punctuations from string library, and a few stop words of my own after looking at a file to see which phrases would best be ignored. Finally, I obtained a list of stop words that was 513 words long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:14:59.464730Z",
     "start_time": "2020-02-13T18:14:56.830974Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation #punctuation is a string made up all punctuations\n",
    "\n",
    "\n",
    "stop = pd.read_csv(\"stop_words.csv\",header=None) #stop_words.csv is a .csv file containing common stop words in english\n",
    "stop = stop.values.tolist() #to convert pandas dataframe to a list (2D list with one column)\n",
    "\n",
    "punctuations = list(punctuation) #to convert string punctuation into a list of strings as ['!','#'..])\n",
    "stopWords = stopwords.words('english')  #stopWords is a list of 179 stop words from nltk \n",
    "\n",
    "\n",
    "stopWords += punctuations  #adding punctuations to list of stop words\n",
    "stopWords+=['subject:','from:', 'date:', 'newsgroups:', 'message-id:', 'lines:', 'path:', 'organization:', \n",
    "            'would', 'writes:', 'references:', 'article', 'sender:', 'nntp-posting-host:', 'people', \n",
    "            'university', 'think', 'xref:', 'cantaloupe.srv.cs.cmu.edu', 'could', 'distribution:', 'first', \n",
    "            'anyone','world', 'really', 'since', 'right', 'believe', 'still', 'keywords','expires','approved'\n",
    "            ,'archive','name','to:','or:','telephone:','fax:','reply-to:',\"that's\",'followup-to:',\"we're\",\n",
    "            \"let's\", \"what's\",\"max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\",\"going\",\"using\",]\n",
    "# adding some more stop words of my own\n",
    "\n",
    "\n",
    "for item in stop: #adding words from csv file not already in stop words list\n",
    "    word = item[0]\n",
    "    if word not in stopWords:\n",
    "        stopWords.append(word)\n",
    "\n",
    "#len(stopWords) #list of stop words is 513 words long "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the vocabulary\n",
    "\n",
    "vocab is a dictionary that is built by going through all files in each folder, word by word. Every word that is of length 5 characters or longer and is not in the list of stop words, is added to vocab as a key, with its value being the total number of times that word has occured in all of the files.\n",
    "\n",
    "\n",
    "In the next cell vocab is sorted in descending order of frequency of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:58.025166Z",
     "start_time": "2020-02-13T18:15:02.626574Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for folder in folders: #fast iteration over list of folders\n",
    "    for doc in data[folder]: #loop over every file in a folder; doc is a string containing entire contents of the file\n",
    "        for word in doc.split(): #splits string doc into a list of strings word wise\n",
    "            if word.lower() not in stopWords and len(word)>=5:\n",
    "                if word.lower() not in vocab: #new words not already present are added as key to vocab with value 1\n",
    "                    vocab[word.lower()] = 1\n",
    "                else:\n",
    "                    vocab[word.lower()] +=1   #update or increment value of keys (words) already present in vocab\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:18:38.415114Z",
     "start_time": "2020-02-13T18:18:38.194474Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sort the dictionary based on frequency of each word in vocab\n",
    "import operator\n",
    "vocab=sorted(vocab.items(),key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "# converts dictionary of key value pairs into a list of tuples as [('word': count) ..]\n",
    "# operator.itemgetter(1) takes the value of the key at index 1 (the count of the word at index 0 of current tuple).\n",
    "# vocab = sorted(vocab.items(), key=lambda x: x[1],reverse = True) can also be used to acheieve the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating list of features\n",
    "\n",
    "The K=2000 most frequently occuring words are chosen as features. As vocab is sorted in descending order of frequency, the top 2000 words are picked up and added to list of features called feature_list.\n",
    "\n",
    "Thus, feature_list is a list of strings which are the 2000 most frequently occuring words in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:18:40.960466Z",
     "start_time": "2020-02-13T18:18:40.947619Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choosing top 2000 vocab words as features\n",
    "feature_list=[]\n",
    "i = 0\n",
    "K = 2000\n",
    "for key in vocab:\n",
    "    feature_list.append(key[0])\n",
    "    i += 1\n",
    "    if i == K:\n",
    "        break       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating X and Y\n",
    "\n",
    "Where Y is a numpy array of output classes (folder name) corresponding to each data point (each file). Y contains 19997 items. \n",
    "\n",
    "df is a pandas dataframe of shape (19997,2000). It uses the chosen features as column labels, with each row corresponding to a file and the number of times a feature word appears in the file is recorded in the column of that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:18:43.017719Z",
     "start_time": "2020-02-13T18:18:42.996459Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Y = []\n",
    "for folder in folders:\n",
    "    for doc in data[folder]:\n",
    "        Y.append(folder)\n",
    "\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code is used to create the dataframe. It takes time to run. This cell can be skipped if pickle is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T11:02:50.438998Z",
     "start_time": "2019-09-17T11:02:48.413Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns = feature_list)\n",
    "for folder in folders: #fast iteration over folders\n",
    "    for file in os.listdir(os.path.join(data_dir,folder)): #loop over every file in the folder\n",
    "        df.loc[len(df)] = np.empty(len(feature_list))  #add a new row for every file: #changed to np.empty from np.zeros\n",
    "        with open(os.path.join(data_dir,folder,file),encoding='latin-1') as opened_file: #to open the file \n",
    "            for word in opened_file.read().split(): #to split string containing the contents of the file word wise, loops over every word\n",
    "                if word.lower() in feature_list:    \n",
    "                    df[word.lower()][len(df)-1] += 1  #df[feature][current_row]\n",
    "                    #if the word is a feature, its count is updated in the column of the feature, in the current row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickling the dataframe\n",
    "\n",
    "The code in the above cell took 45-50 minutes to run. Saved the dataframe so as to not run the code repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T11:02:50.441269Z",
     "start_time": "2019-09-17T11:02:48.417Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.to_pickle('newsgroup.pickle') #dataframe to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:19:12.683479Z",
     "start_time": "2020-02-13T18:19:10.763856Z"
    }
   },
   "outputs": [],
   "source": [
    "# load pickle to kernel\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_pickle('newsgroup.pickle') #to read the pickle into the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T18:06:53.901679Z",
     "start_time": "2019-07-03T18:06:53.898064Z"
    }
   },
   "source": [
    "#### Creating X\n",
    "\n",
    "X is a numpy array of shape (19997, 2000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:19:15.044032Z",
     "start_time": "2020-02-13T18:19:15.038717Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.values #to assign values of the dataframe to X, without index labels or column labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T18:08:56.326112Z",
     "start_time": "2019-07-03T18:08:56.321333Z"
    }
   },
   "source": [
    "## Dividing data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:19:19.390564Z",
     "start_time": "2020-02-13T18:19:17.793406Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:19:19.428589Z",
     "start_time": "2020-02-13T18:19:19.400147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 5., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-built Naive Bayes in sklearn\n",
    "\n",
    " - __score obtained:__ 0.8506 \n",
    " - __average precision:__ 0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:19:22.673104Z",
     "start_time": "2020-02-13T18:19:22.366656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:19:23.293591Z",
     "start_time": "2020-02-13T18:19:23.215023Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred1 = clf.predict(X_test)  #y_pred1 contains values predicted by in-built naive bayes algorithm in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:19:24.075786Z",
     "start_time": "2020-02-13T18:19:24.009092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "#printing score, confusion matrix and classification report\n",
    "print(\"Score: \",clf.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:19:25.952830Z",
     "start_time": "2020-02-13T18:19:25.919430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "\n",
      "[[185   2   0   0   0   0   0   0   0   1   0   0   0   7   2   1   2  10\n",
      "    7  52]\n",
      " [  0 198   9   7   4  11   2   1   0   0   0   1   4   1   5   0   1   0\n",
      "    0   1]\n",
      " [  1   8 216   2   7  13   0   1   0   0   0   2   6   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   2   5 205   6   2   5   2   0   0   0   0   4   1   2   0   1   0\n",
      "    2   1]\n",
      " [  0   7   2   8 208   0   4   1   0   0   1   1   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   5   8   3   0 206   1   0   2   0   0   1   1   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   5   2   7   4   0 230   6   5   2   1   0   2   1   1   0   3   2\n",
      "    2   1]\n",
      " [  1   5   2   2   2   0   4 239   7   2   0   2   6   2   2   0   2   2\n",
      "    3   1]\n",
      " [  0   4   0   1   1   0   2   4 265   0   2   0   1   4   3   0   1   3\n",
      "    8   0]\n",
      " [  1   1   0   0   0   2   0   0   0 243   0   1   0   0   1   0   0   0\n",
      "    2   4]\n",
      " [  1   1   0   0   0   0   1   1   0   0 224   0   0   2   1   0   1   0\n",
      "    1   1]\n",
      " [  0   2   0   1   0   2   1   0   0   0   0 216   0   0   0   0   5   2\n",
      "    4   0]\n",
      " [  1   4   3   4   4   1   6   6   3   0   1   2 213   2   0   0   0   1\n",
      "    0   1]\n",
      " [  2   4   1   0   0   1   0   1   0   0   0   1   2 227   2   2   0   2\n",
      "    6   0]\n",
      " [  1   4   1   0   0   2   2   2   0   0   0   1   1   1 222   0   0   0\n",
      "    3   1]\n",
      " [  2   0   0   0   0   0   0   0   0   0   1   0   0   2   0 249   1   1\n",
      "    0  16]\n",
      " [  0   0   0   0   0   0   1   4   2   0   0   4   0   2   2   0 214   6\n",
      "   39  30]\n",
      " [  1   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0 240\n",
      "   14   2]\n",
      " [  3   0   0   0   0   0   1   1   0   0   1   1   1   2   0   0  10  12\n",
      "  148  20]\n",
      " [ 34   1   0   0   0   0   0   0   0   0   0   0   0   0   2   0   8   0\n",
      "   20 105]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix: \")\n",
    "print()\n",
    "print(confusion_matrix(y_pred1,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:19:28.233628Z",
     "start_time": "2020-02-13T18:19:28.122150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report: \n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.79      0.69      0.74       269\n",
      "           comp.graphics       0.78      0.81      0.80       245\n",
      " comp.os.ms-windows.misc       0.87      0.84      0.85       257\n",
      "comp.sys.ibm.pc.hardware       0.85      0.86      0.86       238\n",
      "   comp.sys.mac.hardware       0.88      0.89      0.88       235\n",
      "          comp.windows.x       0.86      0.90      0.88       229\n",
      "            misc.forsale       0.88      0.84      0.86       274\n",
      "               rec.autos       0.89      0.84      0.86       284\n",
      "         rec.motorcycles       0.93      0.89      0.91       299\n",
      "      rec.sport.baseball       0.98      0.95      0.97       255\n",
      "        rec.sport.hockey       0.97      0.96      0.96       234\n",
      "               sci.crypt       0.93      0.93      0.93       233\n",
      "         sci.electronics       0.87      0.85      0.86       252\n",
      "                 sci.med       0.89      0.90      0.90       251\n",
      "               sci.space       0.90      0.92      0.91       241\n",
      "  soc.religion.christian       0.99      0.92      0.95       272\n",
      "      talk.politics.guns       0.86      0.70      0.77       304\n",
      "   talk.politics.mideast       0.85      0.93      0.89       258\n",
      "      talk.politics.misc       0.57      0.74      0.64       200\n",
      "      talk.religion.misc       0.44      0.62      0.52       170\n",
      "\n",
      "                accuracy                           0.85      5000\n",
      "               macro avg       0.85      0.85      0.85      5000\n",
      "            weighted avg       0.86      0.85      0.85      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report: \")\n",
    "print(classification_report(y_pred1,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Naive Bayes code from scratch: \n",
    "\n",
    "\n",
    " - Average precision: 0.86."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Function\n",
    "\n",
    "Creates a dictionary result as:\n",
    "result = { 'total_data' : 14997, 'class1' : { 'total_count_datapoints\" : total no of datapoints belonging to class1, 'total_count' : total no of feature words belonging to class1, 'feature1' : no of times feature1 appears in all data points belonging to class1, 'feature2' : count of appearance of feature2 in class 1,..},  class2' : {'total count': value, 'feature1': value,...}...\n",
    "}\n",
    "\n",
    "These counts are used to calculate (logarithmic) probabilities and hence determine output classes using Bayes theorem, assuming features to be independent of each other (Naive assumption).\n",
    "\n",
    "This dictionary is used by functions called by predict function to calculate probabilties and hence decide output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:19:33.405924Z",
     "start_time": "2020-02-13T18:19:33.392483Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit(X_train,Y_train):\n",
    "    result = {}\n",
    "    result[\"total_data\"] = len(Y_train) #count of total number of data points (in training set)\n",
    "    class_values = set(Y_train) #set of all possible output classes\n",
    "    \n",
    "    for current_class in class_values: #iterate over every possible class\n",
    "        result[current_class] = {} #value for key current_class is an (empty) dictionary\n",
    "        current_class_rows = (Y_train == current_class) #a true/false array with true values for rows that have output class equal to current_class\n",
    "        X_train_current = X_train[current_class_rows] #rows of X_train belonging to current_class\n",
    "        Y_train_current = Y_train[current_class_rows ] #rows of Y_train belonging to current_Class\n",
    "        result[current_class][\"total_count_datapoints\"] = len(Y_train_current)\n",
    "        num_features = X_train.shape[1] #number of features in x train data (columns)\n",
    "        total_words = 0\n",
    "        for i in range(num_features): #iterate over features\n",
    "            count_feature_in_class = X_train_current[:,i].sum()   #no of times the feature appears in all data points belonging to the current class\n",
    "            result[current_class][feature_list[i]] = count_feature_in_class \n",
    "            total_words+= count_feature_in_class\n",
    "        result[current_class][\"total_count\"]=total_words #total no of feature words in the class\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:19:33.667510Z",
     "start_time": "2020-02-13T18:19:33.657547Z"
    }
   },
   "outputs": [],
   "source": [
    "def probability(dictionary,x,current_class):\n",
    "    output = np.log(dictionary[current_class]['total_count_datapoints']) - np.log(dictionary[\"total_data\"])#logarithmic class probability    for i in range(len(feature_list)):       #iterating over every feature\n",
    "    for i in range(len(feature_list)):\n",
    "        current_word_count = dictionary[current_class][feature_list[i]] + 1  #no of times feature appears in current_class. +1 for laplace correction\n",
    "        total_word_count = dictionary[current_class][\"total_count\"]+len(feature_list)  #no of data points in current class with laplace correction\n",
    "        current_word_probability = np.log(current_word_count) - np.log(total_word_count)\n",
    "        if int(x[i])!= 0: #to ignore words having zero frequency\n",
    "            output += current_word_probability #logarithmic probabilties added instead of being multiplied\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:19:34.013773Z",
     "start_time": "2020-02-13T18:19:34.003992Z"
    }
   },
   "outputs": [],
   "source": [
    "def predictSinglePoint(dictionary,x):\n",
    "    classes = dictionary.keys() #all possible output classes\n",
    "    best_p = -1000\n",
    "    best_class = -1\n",
    "    first_run = True #for first run of loop\n",
    "    for current_class in classes: #iterate over all classes\n",
    "        if current_class == \"total_data\": #to skip total_data key as it doesnt correspond to an output class\n",
    "            continue\n",
    "        p_current_class = probability(dictionary,x,current_class) #probability of current class being the class of the data point\n",
    "        if (first_run or p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "   \n",
    "    return best_class\n",
    "\n",
    "# The probability of the data point belonging to each class is calculated. The class with the highest probability is chosen\n",
    "# to be the output class for the data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:19:34.516881Z",
     "start_time": "2020-02-13T18:19:34.508538Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(dictionary,X_test):\n",
    "    y_pred = []\n",
    "    for x in X_test: #iterate over ever row or data point\n",
    "        x_class = predictSinglePoint(dictionary,x)\n",
    "        y_pred.append(x_class)\n",
    "    \n",
    "    return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:19:36.837328Z",
     "start_time": "2020-02-13T18:19:36.407557Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict function takes time to run. This cell can be skipped. proceed by loading the pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T11:11:07.686011Z",
     "start_time": "2019-09-17T11:09:29.779722Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = predict(dictionary,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T11:02:50.493049Z",
     "start_time": "2019-09-17T11:02:48.477Z"
    }
   },
   "outputs": [],
   "source": [
    "# pickled the output as predict function took time to run\n",
    "# import pickle\n",
    "# with open('predictions.pickle', 'wb') as f:\n",
    "#     pickle.dump(y_pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:19:59.294622Z",
     "start_time": "2020-02-13T18:19:59.285956Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('predictions.pickle','rb') as f:\n",
    "    y_pred = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:20:02.229790Z",
     "start_time": "2020-02-13T18:20:02.175899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "\n",
      "[[184   1   0   0   0   1   0   0   0   0   0   0   0   6   2   0   2  16\n",
      "    8  51]\n",
      " [  0 204   9   5   4  10   3   2   0   0   0   2   2   0   5   0   0   0\n",
      "    2   1]\n",
      " [  1   5 214   2   4  12   1   0   0   0   0   1   5   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   1   5 207   4   2   3   2   0   0   0   0   2   1   1   0   1   0\n",
      "    2   1]\n",
      " [  0   9   3   6 212   0   4   0   0   0   1   1   2   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   4   7   2   1 208   1   0   2   0   0   2   0   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   7   3   7   4   1 229   7   6   1   0   0   2   1   1   0   0   2\n",
      "    2   1]\n",
      " [  2   5   2   4   2   0   6 243   6   2   0   2   5   2   3   0   2   2\n",
      "    2   3]\n",
      " [  0   3   0   2   3   0   2   6 267   0   2   0   0   6   4   0   1   3\n",
      "    5   2]\n",
      " [  1   1   0   0   0   0   0   0   0 245   1   1   0   0   3   0   0   0\n",
      "    2   1]\n",
      " [  1   1   0   0   0   0   1   1   0   0 223   0   0   1   1   0   1   0\n",
      "    1   1]\n",
      " [  0   4   1   2   0   1   1   0   0   0   0 218   0   1   0   0   3   2\n",
      "    4   0]\n",
      " [  1   3   2   3   2   1   5   3   2   0   2   1 222   2   3   0   0   1\n",
      "    0   1]\n",
      " [  2   2   1   0   0   1   0   0   0   0   0   0   2 225   2   2   0   2\n",
      "    2   0]\n",
      " [  1   3   1   0   0   3   2   1   0   0   0   2   1   2 216   0   0   0\n",
      "    2   1]\n",
      " [  2   0   0   0   0   0   0   0   0   0   1   0   0   2   0 250   0   0\n",
      "    0  13]\n",
      " [  0   0   0   0   0   0   1   3   1   0   0   2   0   2   1   0 222   6\n",
      "   42  26]\n",
      " [  1   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0 232\n",
      "   12   2]\n",
      " [  4   0   1   0   0   0   1   1   0   0   1   1   1   3   1   0   8  13\n",
      "  149  23]\n",
      " [ 33   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   9   2\n",
      "   24 109]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix: \")\n",
    "print()\n",
    "print(confusion_matrix(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:20:05.962267Z",
     "start_time": "2020-02-13T18:20:05.855410Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.79      0.68      0.73       271\n",
      "           comp.graphics       0.81      0.82      0.81       249\n",
      " comp.os.ms-windows.misc       0.86      0.87      0.86       246\n",
      "comp.sys.ibm.pc.hardware       0.86      0.89      0.88       232\n",
      "   comp.sys.mac.hardware       0.90      0.89      0.89       239\n",
      "          comp.windows.x       0.87      0.91      0.89       229\n",
      "            misc.forsale       0.88      0.84      0.86       274\n",
      "               rec.autos       0.90      0.83      0.86       293\n",
      "         rec.motorcycles       0.94      0.87      0.91       306\n",
      "      rec.sport.baseball       0.99      0.96      0.97       255\n",
      "        rec.sport.hockey       0.97      0.96      0.96       232\n",
      "               sci.crypt       0.94      0.92      0.93       237\n",
      "         sci.electronics       0.91      0.87      0.89       254\n",
      "                 sci.med       0.88      0.93      0.91       241\n",
      "               sci.space       0.88      0.92      0.90       235\n",
      "  soc.religion.christian       0.99      0.93      0.96       268\n",
      "      talk.politics.guns       0.89      0.73      0.80       306\n",
      "   talk.politics.mideast       0.83      0.94      0.88       248\n",
      "      talk.politics.misc       0.58      0.72      0.64       207\n",
      "      talk.religion.misc       0.46      0.61      0.53       178\n",
      "\n",
      "                accuracy                           0.86      5000\n",
      "               macro avg       0.86      0.85      0.85      5000\n",
      "            weighted avg       0.86      0.86      0.86      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Naive Bayes algorithm that was implemented from scratch was found to have an average precision of 0.86 which is the same as the sklearn multinomial Naive Bayes algorithm. The in-built sklearn implementation had coefficient of determination (score) = 0.8506"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
